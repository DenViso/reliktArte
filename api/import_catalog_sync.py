import sys
import os
import time
import traceback
from pathlib import Path
from dotenv import load_dotenv
from collections import defaultdict
from sqlalchemy import create_engine, select, text
from sqlalchemy.orm import sessionmaker, Session

# –Ü–º–ø–æ—Ä—Ç –≤–∞—à–∏—Ö –º–æ–¥–µ–ª–µ–π —Ç–∞ –µ–Ω—É–º—ñ–≤
from src.product.models import (
    Product, Category, ProductPhoto, 
    ProductSize, ProductColor, ProductGlassColor
)
from src.product.enums import ProductPhotoDepEnum

# –°–ø—Ä–æ–±–∞ —ñ–º–ø–æ—Ä—Ç—É docx
try:
    from docx import Document
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("‚ö†Ô∏è python-docx –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ. –û–ø–∏—Å–∏ –Ω–µ –±—É–¥—É—Ç—å –∑—á–∏—Ç–∞–Ω—ñ.")

# –ì–ª–æ–±–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
STATS = {
    'import_details': defaultdict(lambda: {
        'folders': 0, 'products_added': 0, 'products_updated': 0, 'photos_added': 0
    })
}

# --- 1. –ü–Ü–î–ì–û–¢–û–í–ö–ê –ë–ê–ó–ò –î–ê–ù–ò–• ---

def sync_references(session: Session, category: Category):
    """–ì–∞—Ä–∞–Ω—Ç—É—î –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –±–∞–∑–æ–≤–∏—Ö —Ä–æ–∑–º—ñ—Ä—ñ–≤ —Ç–∞ –∫–æ–ª—å–æ—Ä—ñ–≤ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ—ó"""
    standard_sizes = [
        {"height": 2000, "width": 600, "thickness": 40},
        {"height": 2000, "width": 700, "thickness": 40},
        {"height": 2000, "width": 800, "thickness": 40},
        {"height": 2000, "width": 900, "thickness": 40},
    ]
    
    db_sizes = []
    for s in standard_sizes:
        size = session.query(ProductSize).filter_by(**s).first()
        if not size:
            size = ProductSize(**s)
            session.add(size)
            session.flush()
        db_sizes.append(size)
    
    category.allowed_sizes = db_sizes
    
    default_color = session.query(ProductColor).filter_by(name="–°—Ç–∞–Ω–¥–∞—Ä—Ç").first()
    if not default_color:
        default_color = ProductColor(name="–°—Ç–∞–Ω–¥–∞—Ä—Ç")
        session.add(default_color)
    
    session.flush()
    return db_sizes[0], default_color

# --- 2. –û–ë–†–û–ë–ö–ê –ö–û–ù–¢–ï–ù–¢–£ (–°–ö–õ–û, SUMMARY, SKU) ---

def extract_docx_content(file_path):
    """–ó—á–∏—Ç—É—î —Ç–µ–∫—Å—Ç, –≤–∏–∑–Ω–∞—á–∞—î SKU, —á–∏—Å—Ç–∏–π –æ–ø–∏—Å —Ç–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å —Å–∫–ª–∞"""
    if not DOCX_AVAILABLE or not file_path.exists():
        return "–û–ø–∏—Å –≤—ñ–¥—Å—É—Ç–Ω—ñ–π", [{"value": "–û–ø–∏—Å –≤—ñ–¥—Å—É—Ç–Ω—ñ–π"}], None, False, False, None, "UNKNOWN"

    try:
        doc = Document(file_path)
        lines = [p.text.strip() for p in doc.paragraphs if p.text.strip()]
        if not lines: 
            return "–û–ø–∏—Å –ø–æ—Ä–æ–∂–Ω—ñ–π", [{"value": "–û–ø–∏—Å –ø–æ—Ä–æ–∂–Ω—ñ–π"}], None, False, False, None, "EMPTY"

        details = [{"value": line} for line in lines]
        full_text = " ".join(lines).lower()
        
        # --- SKU: –¢—ñ–ª—å–∫–∏ –ø–µ—Ä—à–∏–π —Ä—è–¥–æ–∫ (–∞—Ä—Ç–∏–∫—É–ª) ---
        extracted_sku = lines[0].strip()

        # --- –ü–û–ö–†–ò–¢–¢–Ø (finishing) ---
        covering = next((line for line in lines if any(kw in line.lower() for kw in ['–ø–≤—Ö', '—à–ø–æ–Ω', '–ª–∞–º—ñ–Ω–∞—Ç', '–¥—É–±'])), None)
        
        # --- –õ–û–ì–Ü–ö–ê –°–ö–õ–ê (—ñ–∑ –∑–∞–ø–µ—Ä–µ—á–µ–Ω–Ω—è–º–∏) ---
        glass_line = next((line for line in lines if any(kw in line.lower() for kw in ['—Å–∫–ª–æ', '—Å–∫–ª—ñ–Ω–Ω—è', '–∑–∞—Å–∫–ª–µ–Ω–∞'])), None)
        has_glass = False
        glass_value = None
        negation_keywords = ['–±–µ–∑', '–Ω–µ –º–∞—î', '–Ω–µ–º–∞—î', '–≤—ñ–¥—Å—É—Ç–Ω—î', '–≤—ñ–¥—Å—É—Ç–Ω—è', '–≥–ª—É—Ö–∞']
        
        if glass_line:
            is_negated = any(neg in glass_line.lower() for neg in negation_keywords)
            if not is_negated:
                has_glass = True
                glass_value = glass_line
        elif '–≥–ª—É—Ö–∞' in full_text:
            has_glass = False

        # –û—Ä—ñ—î–Ω—Ç–∞—Ü—ñ—è
        has_orient = any(kw in full_text for kw in ['–ø—Ä–∞–≤–µ', '–ª—ñ–≤–µ', '–ø—Ä–∞–≤–∏–π', '–ª—ñ–≤–∏–π'])
        
        # --- –ß–ò–°–¢–ò–ô SUMMARY (–¢—ñ–ª—å–∫–∏ –ê—Ä—Ç–∏–∫—É–ª —Ç–∞ –ú–æ–¥–µ–ª—å) ---
        stop_keywords = ['–ø–≤—Ö', '—à–ø–æ–Ω', '–ª–∞–º—ñ–Ω–∞—Ç', '–¥—É–±', '2000', '—Ö', '–ø—Ä–∞–≤–µ', '–ª—ñ–≤–µ', '—Å–∫–ª–∞', '—Å–∫–ª–æ']
        clean_parts = []
        for line in lines[:3]:
            if covering and line == covering: continue
            if any(stop in line.lower() for stop in stop_keywords): continue
            clean_parts.append(line)
            if len(clean_parts) >= 2: break
        summary = " ‚Ä¢ ".join(clean_parts)
        
        return summary, details, covering, has_glass, has_orient, glass_value, extracted_sku
    except Exception:
        return "–ü–æ–º–∏–ª–∫–∞", [], None, False, False, None, "ERROR"

# --- 3. –ê–ù–ê–õ–Ü–ó –¢–ê –Ü–ú–ü–û–†–¢ ---

def analyze_and_import(session: Session, cat_name: str):
    folder_key = "door" if cat_name == "–î–≤–µ—Ä—ñ" else "mouldings"
    base_path = Path(f"static/catalog/{folder_key}")
    
    if not base_path.exists():
        print(f"‚ùå –®–ª—è—Ö –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ: {base_path}")
        return

    cat = session.query(Category).filter_by(name=cat_name).first()
    if not cat:
        cat = Category(name=cat_name, is_glass_available=(cat_name == "–î–≤–µ—Ä—ñ"))
        session.add(cat)
        session.flush()
    
    def_size, def_color = sync_references(session, cat)

    print(f"\nüîç –Ü–ú–ü–û–†–¢: {cat_name.upper()}")
    print("-" * 60)

    product_dirs = []
    if cat_name == "–î–≤–µ—Ä—ñ":
        for class_dir in sorted(base_path.iterdir()):
            if class_dir.is_dir():
                for p_dir in sorted(class_dir.iterdir()):
                    if p_dir.is_dir():
                        product_dirs.append((class_dir.name, p_dir))
    else:
        for p_dir in sorted(base_path.iterdir()):
            if p_dir.is_dir():
                product_dirs.append(("–ë–∞–∑–æ–≤–∞", p_dir))

    for class_name, p_dir in product_dirs:
        photos = list(p_dir.glob('*.webp')) + list(p_dir.glob('*.jpg')) + list(p_dir.glob('*.png'))
        docx_path = p_dir / "description.docx"
        if not photos and not docx_path.exists():
            continue

        summary, details, cover, glass, orient, glass_v, extracted_sku = extract_docx_content(docx_path)
        
        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ —á–∏—Å—Ç–∏–π –∞—Ä—Ç–∏–∫—É–ª —è–∫ SKU
        sku = extracted_sku 
        
        product = session.query(Product).filter_by(sku=sku).first()
        desc_json = {"text": summary, "details": details}
        if cover: desc_json["finishing"] = {"covering": {"text": cover}}

        if not product:
            product = Product(
                sku=sku, category_id=cat.id, 
                price=0,  # 0 = "–ù–∞–¥—ñ—à–ª—ñ—Ç—å –∑–∞–ø–∏—Ç"
                name=f"{class_name} {p_dir.name}",
                description=desc_json,
                have_glass=glass, orientation_choice=orient
            )
            session.add(product)
            session.flush()
            STATS['import_details'][cat_name]['products_added'] += 1
            print(f"  ‚ûï –î–æ–¥–∞–Ω–æ: SKU {sku}")
        else:
            product.description = desc_json
            product.have_glass = glass
            product.price = 0
            product.orientation_choice = orient
            STATS['import_details'][cat_name]['products_updated'] += 1
            print(f"  üîÑ –û–Ω–æ–≤–ª–µ–Ω–æ: SKU {sku}")

        # –û–±—Ä–æ–±–∫–∞ –§–æ—Ç–æ
        existing_photos = {p.photo for p in session.query(ProductPhoto).filter_by(product_id=product.id).all()}
        for idx, p_file in enumerate(sorted(photos)):
            web_path = f"/static/catalog/{folder_key}/{p_dir.relative_to(base_path)}/{p_file.name}"
            if web_path not in existing_photos:
                new_photo = ProductPhoto(
                    photo=web_path, product_id=product.id,
                    is_main=(idx == 0),
                    dependency=ProductPhotoDepEnum.COLOR,
                    color_id=def_color.id, size_id=def_size.id,
                    with_glass=glass_v
                )
                session.add(new_photo)
                STATS['import_details'][cat_name]['photos_added'] += 1

# --- 4. –ó–ê–ü–£–°–ö ---

def main():
    load_dotenv('.env')
    db_url = os.getenv('DATABASE_URL')
    if not db_url:
        print("‚ùå DATABASE_URL –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
        return
    
    db_url = db_url.replace('postgresql://', 'postgresql+psycopg2://')
    engine = create_engine(db_url, pool_pre_ping=True)
    SessionLocal = sessionmaker(bind=engine)
    
    with SessionLocal() as session:
        try:
            analyze_and_import(session, "–î–≤–µ—Ä—ñ")
            analyze_and_import(session, "–õ–∏—à—Ç–≤–∏")
            session.commit()
            print("\n‚úÖ –£–°–ü–Ü–®–ù–û –ó–ê–í–ï–†–®–ï–ù–û")
        except Exception as e:
            session.rollback()
            print(f"\n‚ùå –ü–û–ú–ò–õ–ö–ê: {e}")
            traceback.print_exc()

if __name__ == "__main__":
    main()